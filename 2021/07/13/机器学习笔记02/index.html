

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/tx.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Brianyjh">
  <meta name="keywords" content="">
  
    <meta name="description" content="01 引言(多元线性回归) “多元线性回归本质上与单元变量线性回归没太大区别” 增添更多特征后，我们引入一系列新的注释： 引入多元变量后，我们直接把原来的函数 简写为 ## 多元线性回归 #### 梯度下降  多元变量的线性回归的梯度下降与一元的没什么区别  梯度下降技巧01-特征缩放 在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习日记02">
<meta property="og:url" content="http://example.com/2021/07/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B002/index.html">
<meta property="og:site_name" content="Brianyjh的私人小站">
<meta property="og:description" content="01 引言(多元线性回归) “多元线性回归本质上与单元变量线性回归没太大区别” 增添更多特征后，我们引入一系列新的注释： 引入多元变量后，我们直接把原来的函数 简写为 ## 多元线性回归 #### 梯度下降  多元变量的线性回归的梯度下降与一元的没什么区别  梯度下降技巧01-特征缩放 在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210712221148700.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210713103010262.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210713105341290.jpg">
<meta property="article:published_time" content="2021-07-13T05:48:40.000Z">
<meta property="article:modified_time" content="2022-10-04T11:21:10.000Z">
<meta property="article:author" content="Brianyjh">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/20210712221148700.png">
  
  
  
  <title>机器学习日记02 - Brianyjh的私人小站</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Brian Yang</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/kl.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="机器学习日记02"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2021-07-13 13:48" pubdate>
          2021年7月13日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          5.4k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          46 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">机器学习日记02</h1>
            
            
              <div class="markdown-body">
                
                <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="引言多元线性回归">01 引言(多元线性回归)</h1>
<p>“多元线性回归本质上与单元变量线性回归没太大区别”
增添更多特征后，我们引入一系列新的注释：<img
src="https://img-blog.csdnimg.cn/20210712211035495.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> 引入多元变量后，我们直接把原来的函数<img
src="https://img-blog.csdnimg.cn/20210712211129836.png" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> 简写为<img
src="https://img-blog.csdnimg.cn/20210712211501466.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> ## 多元线性回归 #### 梯度下降</p>
<blockquote>
<p>多元变量的线性回归的梯度下降与一元的没什么区别</p>
</blockquote>
<h4 id="梯度下降技巧01-特征缩放">梯度下降技巧01-特征缩放</h4>
<p>在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。</p>
<p>以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为0-2000平方英尺，而房间数量的值则是
0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。</p>
<p>解决的方法是尝试将所有特征的尺度都尽量缩放到-1到 1之间。如图： <img
src="https://img-blog.csdnimg.cn/20210712211930759.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /><img
src="https://img-blog.csdnimg.cn/20210712212004964.jpg" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /></p>
<h4
id="梯度下降技巧02-学习率与终止迭代条件">梯度下降技巧02-学习率与终止迭代条件</h4>
<p>梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，我们可以绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛。</p>
<p>也有一些自动测试是否收敛的方法，例如将代价函数的变化值与某个阀值（例如
0.001进行比较，但通常看上面这样的图表更好。）如笔记01的代码的终止条件</p>
<p><strong>学习率的选择</strong> <img
src="https://img-blog.csdnimg.cn/20210712212237449.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /></p>
<h2 id="用多项式回归模型">用多项式回归模型</h2>
<h1 id="normal-equation法">02 Normal Equation法</h1>
<p>到目前为止，我们都在使用梯度下降算法，但是对于某些线性回归问题，正规方程（Normal
Equation）方法是更好的解决方案。如： <img
src="https://img-blog.csdnimg.cn/20210712212511688.png" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /><img
src="https://img-blog.csdnimg.cn/20210712212540238.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /></p>
<h4 id="推导过程">推导过程</h4>
<p><span class="math inline">\(\theta=\left(X^{T} X\right)^{-1} X^{T}
y\)</span>的推导过程： <span class="math inline">\(J(\theta)=\frac{1}{2
m}
\sum\limits_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}
\text { 其中: } h_{\theta}(x)=\theta^{T} X=\theta_{0} x_{0}+\theta_{1}
x_{1}+\theta_{2} x_{2}+\ldots+\theta_{n} x_{n}\)</span></p>
<p>将向量表达形式转为矩阵表达形式, 则有 <span
class="math inline">\(J(\theta)=\frac{1}{2}(X \theta-y)^{2}\)</span>,
其中 <span class="math inline">\(X\)</span> 为 <span
class="math inline">\(m\)</span> 行 <span
class="math inline">\(n\)</span> 列的矩阵 <span
class="math inline">\((m\)</span> 为样本个数, <span
class="math inline">\(n\)</span> 为特征个数）, <span
class="math inline">\(\theta\)</span> 为 <span
class="math inline">\(n\)</span> 行 1 列的矩阵, <span
class="math inline">\(y\)</span> 为 <span
class="math inline">\(m\)</span> 行 1 列的矩阵, 对 <span
class="math inline">\(J(\theta)\)</span> 进行如 下变换: <span
class="math display">\[\begin{array}{c}
J(\theta)=\frac{1}{2}(X \theta-y)^{T}(X \theta-y) \\
\quad=\frac{1}{2}\left(\theta^{T} X^{T}-y^{T}\right)(X \theta-y) \\
=\frac{1}{2}\left(\theta^{T} X^{T} X \theta-\theta^{T} X^{T} y-y^{T} X
\theta-y^{T} y\right)
\end{array}\]</span> 接下来对 𝐽(𝜃)偏导，需要用到以下几个矩阵的求导法则 :
<span class="math inline">\(\frac{dAB}{dB}=A^T\)</span></p>
<p><span class="math inline">\(\frac{dX^{\tau}AX}{dX}=2AX\)</span></p>
<p>所以有 : <span class="math display">\[\begin{array}{c}
\frac{\partial J(\theta)}{\partial \theta}=\frac{1}{2}\left(2 X^{T} X
\theta-X^{T} y-\left(y^{T} X\right)^{T}-0\right) \\
=\frac{1}{2}\left(2 X^{T} X \theta-X^{T} y-X^{T} y-0\right) \\
=X^{T} X \theta-X^{T} y
\end{array}\]</span> <span class="math inline">\(\begin{array}{l}
\text { 令 } \frac{\partial J(\theta)}{\partial \theta}=0 \\
\text { 则有 } \theta=\left(X^{T} X\right)^{-1} X^{T} y
\end{array}\)</span></p>
<h2 id="normal-equation法与梯度下降法比较">Normal
Equation法与梯度下降法比较</h2>
<figure>
<img
src="https://img-blog.csdnimg.cn/20210712212645455.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" />
<figcaption aria-hidden="true">在这里插入图片描述</figcaption>
</figure>
<p>总结一下，只要特征变量的<strong>数目并不大</strong>，标准方程是一个很好的计算参数𝜃的<strong>替代方法</strong>。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用<strong>梯度下降法</strong>。</p>
<p>随着我们要讲的学习算法越来越复杂，例如，当我们讲到分类算法，像逻辑回归算法，我们会看到，实际上对于那些算法，<strong>并不能使用标准方程法</strong>。对于那些更复杂的学习算法，我们将不得不仍然使用梯度下降法。因此，<strong>梯度下降法是一个非常有用的算法</strong>，可以用在有大量特征变量的线性回归问题。或者我们以后在课程中，会讲到的一些其的算法，因为标准方程法不适合或者不能用在它们上。但对于这个特定的线性回归模型，标准方程法是一个比梯度下降法更快的替代算法。所以，根据具体的问题，以及你的特征变量的数量，这两种算法都是值得学习的。</p>
<h1 id="logistics回归logistic-regression">03 logistics回归(Logistic
Regression)</h1>
<h2 id="开始">开始</h2>
<p>步骤： 1.确定目标与条件 我是要进行分类吗？（二分类？多分类？）
二分类直接使用，多分类（需要修改） 条件</p>
<p>2.通过梯度下降算得</p>
<h2 id="分类问题引言">分类问题引言</h2>
<p>分类问题中，预测的变量 𝑦 是离散的值</p>
<p>二元的分类问题：负向类（negative class）和正向类（positive
class）</p>
<p>如果我们要用线性回归算法来解决一个分类问题，会出问题</p>
<p>在逻辑回归中，h（假设）的方程换了一个形式 希望我们的分类器的输出值在
0和 1之间，因此逻辑回归模型的假设是： <img
src="https://img-blog.csdnimg.cn/20210712215942320.jpg" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> 𝑋 代表特征向量𝑔 代表逻辑函数（ logistic
function)是一个常用的逻辑函数为是一个常用的逻辑函数为S形函数（形函数（Sigmoid
function），公式为：<img
src="https://img-blog.csdnimg.cn/20210712220058887.jpg" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> #### sigmoid函数 该函数的图像为： <img
src="https://img-blog.csdnimg.cn/20210712220146585.png" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /></p>
<h4 id="判定边界">判定边界</h4>
<p>现在讲下决策边界(decision
boundary)的概念。这个概念能更好地帮助我们理解逻辑回归的假设函数在计算什么。</p>
<blockquote>
<p>假设函数计算内容</p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/20210712221148700.png" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> <img
src="https://img-blog.csdnimg.cn/20210712221242157.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> 现在假设我们有一个模型： <img
src="https://img-blog.csdnimg.cn/20210712221345897.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> <img
src="https://img-blog.csdnimg.cn/20210712221505277.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> <img
src="https://img-blog.csdnimg.cn/20210712221518892.png" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /></p>
<h4 id="逻辑回归函数回顾">逻辑回归函数回顾</h4>
<p><img
src="https://img-blog.csdnimg.cn/2021071222184955.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> #### 代价函数</p>
<blockquote>
<p>逻辑回归的代价函数与线性回归的不一样</p>
<p><img
src="https://img-blog.csdnimg.cn/20210712222038385.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> <img
src="https://img-blog.csdnimg.cn/20210713085554985.png" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /></p>
</blockquote>
<p><em>这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。</em></p>
<p><img
src="https://img-blog.csdnimg.cn/20210712222148817.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> <img
src="https://img-blog.csdnimg.cn/20210712222332118.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> #### 函数代码</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs matlab">clc;clear;<br>x1 = <span class="hljs-number">0.4999999999</span>:<span class="hljs-number">-0.01</span>:<span class="hljs-number">0</span><br>x2 = <span class="hljs-number">0.5</span>:<span class="hljs-number">0.01</span>:<span class="hljs-number">0.99</span><br><br>y1= -<span class="hljs-built_in">log</span>(<span class="hljs-number">1</span>-x1)<br>y2= -<span class="hljs-built_in">log</span>(x2)<br><span class="hljs-built_in">plot</span>(x1,y1,<span class="hljs-string">&#x27;b*&#x27;</span>)<br><span class="hljs-built_in">hold</span> on<br><span class="hljs-built_in">plot</span>(x2,y2,<span class="hljs-string">&#x27;r-&#x27;</span>)<br></code></pre></td></tr></table></figure>
<p><img
src="https://img-blog.csdnimg.cn/20210712224212827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> 进一步简化 <img
src="https://img-blog.csdnimg.cn/20210712224426351.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">cost</span>(<span class="hljs-params">theta, X, y</span>):<br>	theta=np.matrix(theta)<br>	X = np.matrix(X)<br>	y = np.matrix(y)<br>	first = np.multiply(-y, np.log(sigmoid(X*theta.T)))<br>	second = np.multiply(( <span class="hljs-number">1</span>-y), np.log( <span class="hljs-number">1</span>-sigmoid(X*theta.T)))<br><span class="hljs-keyword">return</span> np. <span class="hljs-built_in">sum</span> (first - second) /(<span class="hljs-built_in">len</span> (X))<br></code></pre></td></tr></table></figure>
<h4 id="梯度下降">梯度下降</h4>
<blockquote>
<p>逻辑回归的梯度下降与线性回归的梯度下降是不一样的</p>
</blockquote>
<p><img
src="https://img-blog.csdnimg.cn/20210712224820275.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> 梯度下降推导 <img
src="https://img-blog.csdnimg.cn/20210713093319366.jpg" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> <img
src="https://img-blog.csdnimg.cn/20210713093410428.jpg" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> <img
src="https://img-blog.csdnimg.cn/20210713093435350.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> <img
src="https://img-blog.csdnimg.cn/20210713093628583.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" />
注：虽然得到的梯度下降算法表面上看上去与线性回归的梯度下降算法一样，但是这里的
ℎ𝜃(𝑥)=𝑔(𝜃𝑇𝑋)与线性回归中不同，所以实际上是不一样的。另外，在运行梯度下降算法之前，进行特征缩放依旧是非常必要的。</p>
<p>一些梯度下降算法之外的选择：除了梯度下降算法以外，还有一些常被用来令代价函数最小的算法，这些算法更加复杂和优越，而且通常不需要人工选择学习率，通常比梯度下降算法要更加快速。这些算法有：
共轭梯度 Conjugate Gradient 局部优化法 (Broyden fletcher goldfarb
shann,BFGS)和 有限内存局部优化法 (LBFGS) fminunc是 matlab和 octave
中都带的一个最小值优化函数，使用时我们需要提供代价函数和每个参数的求导
#### 简化的损失函数和梯度下降</p>
<blockquote>
<p>找出一种稍微简单一点的方法来写代价函数，来替换我们现在用的方法。弄清楚如何运用梯度下降法，来拟合出逻辑回归的参数。</p>
</blockquote>
<p>这就是逻辑回归的代价函数： <img
src="https://img-blog.csdnimg.cn/20210713094416207.png" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> 这个式子可以合并成： <img
src="https://img-blog.csdnimg.cn/20210713094513727.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" />
就是这样，现在你知道如何实现逻辑回归，这是一种非常强大，甚至可能世界上使用最广泛的一种分类算法。</p>
<h4 id="高级优化">高级优化</h4>
<blockquote>
<p>学一些高级优化算法和一些高级的优化概念，利用这些方法，我们就能够使通过梯度下降，进行逻辑回归的速度大大提高，而这也将使算法更加适合解决大型的机器学习问题</p>
</blockquote>
<h4 id="一对多分类">一对多分类</h4>
<blockquote>
<p>有几种情况就制作几个分类器，每个分类器进行一次二分类，选择最高
的概率值，我们预测 𝑦就 是那个值</p>
</blockquote>
<p>你现在知道了基本的挑选分类器的方法，选择出哪一个分类器是可信度最高效果最好的，
那么就可认为得到一个正确的分类，无论 𝑖值是多少，我们都有最高
的概率值，我们预测 𝑦就
是那个值。这就是多类别分类问题，以及一对多的方法，通过这个小方法，你现在也可以将
逻辑回归分类器用在多类分类的问题上。 # 04 正则化(Regularization) ##
过拟合与欠拟合
第一个模型是一个线性模型，欠拟合，不能很好地适应我们的训练集；第三个模型是一个四次方的模型，过于强调拟合原始数据，而丢失了算法的本质：预测新数据。我们可以
看出，若给出一个新的值使之预测，它将表现的很差，是过拟合，虽然能非常好地适应我们的训练集但在新输入变量进行预测时可能会效果不好；而中间的模型似乎最合适。
<img
src="https://img-blog.csdnimg.cn/20210713102559266.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" />
问题是，如果我们发现了过拟合问题，应该如何处理？ <img
src="https://img-blog.csdnimg.cn/20210713102637950.png" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> ## 代价函数</p>
<blockquote>
<p>高次项导致了过拟合的产生，是高次项的系数小一点接近于0，可以解决过拟合问题</p>
</blockquote>
<blockquote>
<p>所以我们要做的就是在一定程度上==减小===这些参数𝜃
的值，这就是正则化的<strong>基本方法</strong>。</p>
</blockquote>
<figure>
<img src="https://img-blog.csdnimg.cn/20210713103010262.png" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" />
<figcaption aria-hidden="true">在这里插入图片描述</figcaption>
</figure>
<p>如上，我们决定要减少 𝜃3和
𝜃4的大小，我们要做的便是==修改代价函数==，在其中 𝜃3和 𝜃4
设置一点惩罚。这样做的话，我们在尝试最小化代价时也需要将这个惩罚纳入考虑中，并最终导致选择较小一些的
𝜃3和 𝜃4。</p>
<p>修改后的代价函数如下： <img
src="https://img-blog.csdnimg.cn/20210713103350720.png" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /></p>
<blockquote>
<p>这是在==明确惩罚对象==的情况下设置的</p>
</blockquote>
<p>但假如我们有非常多的特征，我们==并不知道==其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过
拟合问题的假设： <img
src="https://img-blog.csdnimg.cn/20210713103702417.jpg" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> 其中𝜆又称为正则化参数（ Regularization
Parameter）。 注：根据惯例，我们不对 𝜃<sub>0</sub> 进
行惩罚。经过正则化处理的模型与原模型的可能对比如下图所示： <img
src="https://img-blog.csdnimg.cn/20210713103854686.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> 如果选择的正则化参数λ过大
则会把所有的参数都最小化了 导致模型变成
ℎ<sub>𝜃(𝑥)</sub>=𝜃<sub>0</sub>，也就是上图中红色直线所示的情况，造成==欠拟合==。</p>
<blockquote>
<p>那为什么增加的一项𝜆=Σ<sub>𝑗=1</sub><sup>n</sup>
𝜃<sub>𝑗</sub><sup>2</sup>以使 𝜃的值减小呢？</p>
</blockquote>
<p>因为如果我们令 𝜆 的值很大的话，为了使 代价函数尽可能的小，所有的 𝜃
的值（不包括 𝜃<sub>0</sub>）都会在一定程度上减小。 但若λ的值太大了 那么
𝜃（不包括 𝜃<sub>0</sub>）都会趋近于 0，这样我们所得到的只能是一条平行于
𝑥轴的直线。</p>
<p>所以对于正则化，我们要取一个合理的𝜆
的值，这样才能更好的应用正则化。</p>
<h2 id="正则化线性回归">正则化线性回归</h2>
<blockquote>
<p>对于线性回归的求解，我们之前推导了两种学习算法：一种基于梯度下降，一种基于正规方程。</p>
</blockquote>
<h4 id="梯度下降-1">梯度下降</h4>
<p>对于基于正则化的梯度下降 <img
src="https://img-blog.csdnimg.cn/20210713104804853.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> 整理得到 <img
src="https://img-blog.csdnimg.cn/20210713104931406.png" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> #### 正规方程
我们同样也可以利用正规方程来求解正则化线性回归模型，方法如下所示： <img
src="https://img-blog.csdnimg.cn/20210713105130272.png" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /> 矩阵大小图中的矩阵尺寸为(𝑛+1)∗(𝑛+1)</p>
<h2 id="正则化的逻辑回归模型">正则化的逻辑回归模型</h2>
<blockquote>
<p>针对逻辑回归问题，我们在之前的课程已经学习过两种优化算法：我们首先学习了使用梯度下降法来优化代价函数
𝐽(𝜃)，接下来学习了更高级的
优化算法，这些高级优化算法需要你自己设计代价函数 𝐽(𝜃)。</p>
</blockquote>
<p>自己计算导数同样对于逻辑回归，我们也给代价函数增加一个正则化的表达式，得到代价函数：
<img src="https://img-blog.csdnimg.cn/20210713105341290.jpg" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">costReg</span>(<span class="hljs-params">theta, X, y, learningRate</span>):<br>	theta=np.matrix(theta)<br>	X=np.matrix(X)<br>	y=np.matrix(y)<br>	first=np.multiply(-y, np.log(sigmoid(X-theta.T)))<br>	second=np.multiply((<span class="hljs-number">1</span>-y), np.log(<span class="hljs-number">1</span>-sigmoid(X theta.T)))<br>	reg=(learningRate/(<span class="hljs-number">2</span>*<span class="hljs-built_in">len</span>(x))*np.<span class="hljs-built_in">sum</span>(np.power(theta[ : , <span class="hljs-number">1</span>:theta. shape[<span class="hljs-number">1</span>]],<span class="hljs-number">2</span>))<br><span class="hljs-keyword">return</span> np.<span class="hljs-built_in">sum</span>(first-second) <span class="hljs-built_in">len</span> (X))+reg<br></code></pre></td></tr></table></figure>
<p>要最小化该代价函数，通过求导，得出梯度下降算法为： <img
src="https://img-blog.csdnimg.cn/20210713105816955.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTUzNjkzNg==,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" lazyload
alt="在这里插入图片描述" />
注：看上去同线性回归一样，但是知道ℎ𝜃(𝑥)=𝑔(𝜃𝑇𝑋)，所以与线性回归不同。</p>
<p>注意： 1.
虽然正则化的逻辑回归中的梯度下降和正则化的线性回归中的表达式看起来一样，
但由于两者的 ℎ𝜃(𝑥)不同所以还是有很大差别。 2.
𝜃0不参与其中的任何一个正则化。</p>
<p>接下来的课程中，我们将学习一个非常强大的非线性分类器，无论是线性回归问题，还
是逻辑回归问题，都可以构造多项式来解决。你将逐渐发现还有更强大的非线性分类器，可
以用来解决多项式回归问题。我们接下来将学会，比现在解决问题的方法强大
N倍的学习 算法。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">#机器学习</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>机器学习日记02</div>
      <div>http://example.com/2021/07/13/机器学习笔记02/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Brianyjh</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2021年7月13日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
                <a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="NC - 非商业性使用">
                    <i class="iconfont icon-nc"></i>
                  </span>
                </a>
              
                <a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="SA - 相同方式共享">
                    <i class="iconfont icon-sa"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/07/26/Linux%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B002/" title="linux学习笔记02">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">linux学习笔记02</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2021/07/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B001/" title="机器学习日记01">
                        <span class="hidden-mobile">机器学习日记01</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Brianyjh</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> <div style="font-size: 0.85rem"> <span id="website-duration">载入网站运行时间...</span> <script src="/js/duration.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script> <span><br></span>	
<span>总访客&nbsp<span id="busuanzi_value_site_uv"></span>&nbsp人 &nbsp</span>
<span>访问总量&nbsp<span id="busuanzi_value_site_pv"></span>&nbsp次</span> </div>   
    </div>
  
  
  
    <!-- 备案信息 ICP for China -->
    <div class="beian">
  <span>
    <a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">
      浙ICP证912321号
    </a>
  </span>
  
    
      <span>
        <a
          href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=93021872"
          rel="nofollow noopener"
          class="beian-police"
          target="_blank"
        >
          
            <span style="visibility: hidden; width: 0">|</span>
            <img src="/img/police_beian.png" srcset="/img/loading.gif" lazyload alt="police-icon"/>
          
          <span>浙公网安备93021872号</span>
        </a>
      </span>
    
  
</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
